{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "data_set_url = 'https://static-content.springer.com/esm/art%3A10.1038%2Fncomms5212/MediaObjects/41467_2014_BFncomms5212_MOESM1045_ESM.txt'\n",
    "data = urlopen(data_set_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = []\n",
    "for line in data:\n",
    "    data_row = line.decode().rstrip()\n",
    "    my_data.append([term for term in data_row.split('\\t')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MeSH Symptom Term', 'MeSH Disease Term', 'PubMed occurrence',\n",
       "       'TFIDF score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_records(my_data[1:], columns = my_data[0])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unique Diseases: 4219\n",
      "# Unique Symptoms: 322\n"
     ]
    }
   ],
   "source": [
    "df.columns = ['symptom','disease','n','score']\n",
    "df['symptom'] = df['symptom'].astype('category')\n",
    "df['disease'] = df['disease'].astype('category')\n",
    "df['n'] = df['n'].astype('int')\n",
    "df['score'] = df['score'].astype('float')\n",
    "df = df.sort_values(by=['disease', 'symptom', 'n', 'score'])\n",
    "possible_diseases = set(df['disease'])\n",
    "possible_symptoms = set(df['symptom'])\n",
    "print('# Unique Diseases: {}'.format(len(possible_diseases)))\n",
    "print('# Unique Symptoms: {}'.format(len(possible_symptoms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('symptom_disease_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['symptom_encod'] = df['symptom'].cat.codes\n",
    "df['disease_encod'] = df['disease'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symptom</th>\n",
       "      <th>disease</th>\n",
       "      <th>n</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Language Development Disorders</td>\n",
       "      <td>22q11 Deletion Syndrome</td>\n",
       "      <td>1</td>\n",
       "      <td>2.486567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mental Retardation</td>\n",
       "      <td>22q11 Deletion Syndrome</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Olfaction Disorders</td>\n",
       "      <td>22q11 Deletion Syndrome</td>\n",
       "      <td>1</td>\n",
       "      <td>2.288230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Respiratory Sounds</td>\n",
       "      <td>22q11 Deletion Syndrome</td>\n",
       "      <td>1</td>\n",
       "      <td>1.639269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Virilism</td>\n",
       "      <td>46, XX Disorders of Sex Development</td>\n",
       "      <td>1</td>\n",
       "      <td>2.227056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          symptom                              disease  n  \\\n",
       "0  Language Development Disorders              22q11 Deletion Syndrome  1   \n",
       "1              Mental Retardation              22q11 Deletion Syndrome  1   \n",
       "2             Olfaction Disorders              22q11 Deletion Syndrome  1   \n",
       "3              Respiratory Sounds              22q11 Deletion Syndrome  1   \n",
       "4                        Virilism  46, XX Disorders of Sex Development  1   \n",
       "\n",
       "      score  \n",
       "0  2.486567  \n",
       "1  0.905447  \n",
       "2  2.288230  \n",
       "3  1.639269  \n",
       "4  2.227056  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('symptom_disease_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df)\n",
    "diseases = list(set(df['disease']))\n",
    "diseases_csv = pd.DataFrame([['disease_id'] + diseases])\n",
    "diseases_csv.to_csv('disease_ids.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disease To Symptom Classifier\n",
    "\n",
    "## Path To Completion\n",
    "\n",
    "1. Reformat data-frame to have symp_1, ..., symp_n, disease_i for each row, with the values being the occurence of symptom j within 1 <= j <= n for disease i.\n",
    "\n",
    "2. Create simulated training and testing data for each disease which match original data's original frequency distribution\n",
    "\n",
    "3. Run random forests model on data\n",
    "\n",
    "## Step 1 - Reformat Data-Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptom_encod = set(df['symptom_encod'])\n",
    "new_col_names = [\"\"]*len(symptom_encod)\n",
    "\n",
    "for i, symp_i in enumerate(symptom_encod):\n",
    "    new_col_names[i] = \"symp_\" + str(symp_i)\n",
    "\n",
    "new_col_names.append(\"disease\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_encod = set(df['disease_encod'])\n",
    "new_df = pd.DataFrame([[0]*len(new_col_names) for _ in range(len(disease_encod))], columns=new_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test grabbing 'n'\n",
    "df[(df.disease_encod == 1) & (df.symptom_encod == 312)][['n']].iloc[0]['n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = len(new_df.columns)\n",
    "def edit_row(rnge):\n",
    "    start, end = rnge\n",
    "    for disease in range(start, end+1):\n",
    "        new_row = [0]*num_cols\n",
    "        new_row[-1] = disease\n",
    "        \n",
    "        for j in range(num_cols-1):\n",
    "            res = df[(df['disease_encod'] == disease) & (df['symptom_encod'] == j)]\n",
    "            if len(res) > 0:\n",
    "                new_row[j] = res[['n']].iloc[0]['n']\n",
    "\n",
    "        # write over row in df\n",
    "        new_df.iloc[disease] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment for re-processing\n",
    "import threading\n",
    "\n",
    "def start_threads(threads):\n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "\n",
    "def join_threads(threads):\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "def multi_thread(n_rows, n_threads, proc):\n",
    "    if n_threads < 1:\n",
    "        return\n",
    "    \n",
    "    jump = n_rows // n_threads\n",
    "    \n",
    "    threads = []\n",
    "    for i in range(0, n_rows, jump):\n",
    "        # split range -> [start, end] --> feed to proc\n",
    "        start, end = i, i+jump\n",
    "        \n",
    "        # extend for final thread\n",
    "        if i // jump == n_threads:\n",
    "            end = n_rows-1\n",
    "            \n",
    "        t = threading.Thread(target=proc, args=((start,end),))\n",
    "        threads.append(t)\n",
    "        \n",
    "    start_threads(threads)\n",
    "    join_threads(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creates the re-formatted data-frame!\n",
    "# multi_thread(len(new_df), 4, edit_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment when processing again\n",
    "# new_df.to_csv('symptom_disease_dataset_reformatted.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "new_df = pd.read_csv('symptom_disease_dataset_reformatted.csv')\n",
    "\n",
    "# make columns floating point data types\n",
    "for col in new_df.columns[:-1]:\n",
    "    new_df[col] = new_df[col].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "#X_train.reshape(-1,1)\n",
    "def model_predict(clf, X_test):\n",
    "    return clf.predict(X_test)\n",
    "\n",
    "def print_accuracy(y_test, y_pred):\n",
    "    print(\"Accuracy: {}\".format(metrics.accuracy_score(y_test, y_pred)))\n",
    "\n",
    "    \n",
    "def build_model(df):\n",
    "    # capturing predictors and predicted \n",
    "    x_cols = df.columns[:-1].tolist()\n",
    "    X = df[pd.Index(x_cols)]\n",
    "    y = df['disease']\n",
    "\n",
    "    # performing 80/20 split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "    \n",
    "    # initializingclf\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=30)\n",
    "    \n",
    "    # Fitting model\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf, X_test, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: Artificially Augment the data\n",
    "- 1. Identify distribution of data\n",
    "- 2. Randomly create new data and append to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def bar_plot(x, y):\n",
    "    fig = go.Figure([go.Bar(x=x,y=y)])\n",
    "    fig.show()\n",
    "\n",
    "def disease_symptom_dist(disease_id):\n",
    "    disease = new_df[new_df['disease'] == disease_id]\n",
    "    y_vals = disease[disease.columns[:-1]].iloc[0].tolist()\n",
    "    \n",
    "    # create bar chart\n",
    "    bar_plot(disease.columns[:-1], y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def apply_noise(df, mu=0, sigma=0.1, round=True):\n",
    "    n_rows, n_cols = len(df), len(df.columns)\n",
    "    noise = np.random.normal(mu, sigma, [n_rows, n_cols])\n",
    "    return df + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not good enough\n",
    "# capture mu and sigma for each symptom\n",
    "def get_mu(col):\n",
    "    return sum(col)/len(col) if len(col) else 0\n",
    "\n",
    "def get_sigma(col, mu=0.1):\n",
    "    n = len(col)\n",
    "    return sum((col-mu)**2)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite new_df by taking row proportions\n",
    "def row_proportion(row):\n",
    "    row_sum = sum(row[:-1])\n",
    "    for i, item in enumerate(row[:-1]):\n",
    "        row[i] = item*1.0 / (row_sum if row_sum else 1)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_row_proportion(bounds):\n",
    "    start, end = bounds\n",
    "    for i in range(start, end+1):\n",
    "        new_df.iloc[i] = row_proportion(new_df.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tik = time.clock()\n",
    "### Applies row proportions\n",
    "# multi_thread(len(new_df), 5, apply_row_proportion)\n",
    "tok = time.clock()\n",
    "print('Total time: {}'.format(tok-tik))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.to_csv('symptom_disease_dataset_row_proportions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import row proportions\n",
    "#\n",
    "# EXECUTE THIS DOWN FOR ARTIFICAL DATA CREATION\n",
    "#\n",
    "#\n",
    "import pandas as pd\n",
    "new_df = pd.read_csv('symptom_disease_dataset_row_proportions.csv')\n",
    "\n",
    "for col in new_df.columns[:-1]:\n",
    "    new_df[col] = new_df[col].astype('float')\n",
    "    \n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row-proportions complete!\n",
    "\n",
    "Now what?\n",
    "\n",
    "Artificial data creation...\n",
    "\n",
    "Let's say for a given disease, we have proportions: `[0.1, 0.2, 0.7]`.\n",
    "\n",
    "We randomly sample a number between 1-1000 for each symptom, we get: `[100, 250, 400]`\n",
    "\n",
    "Now apply the proportions: `[10, 50, 280]`\n",
    "\n",
    "Recalculate the row-proportions: `[.027, 0.205, .788]` This is our new row of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_artificial_row(row):\n",
    "    n = len(row)\n",
    "    artificial_row = [0.0]*n\n",
    "    \n",
    "    # create new artificial counts\n",
    "    for i, actual_proportion in enumerate(row[:-1]):\n",
    "        artificial_row[i] = random.randint(0,1000) * actual_proportion\n",
    "    \n",
    "    # add disease to artificial row\n",
    "    artificial_row[-1] = row[-1]\n",
    "    \n",
    "    return row_proportion(artificial_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking good, let's multithread and enhance!!\n",
    "\n",
    "# create massive data frame, where each piece of data has 500 additional rows of artificial data\n",
    "N_ADDITIONAL = 200\n",
    "\n",
    "# stretch new_df\n",
    "old_n_rows, old_n_cols = len(new_df), len(new_df.columns)\n",
    "new_n_rows = old_n_rows + old_n_rows * N_ADDITIONAL\n",
    "\n",
    "\n",
    "def apply_artificial_data(bounds):\n",
    "    print(bounds)\n",
    "    start, end = bounds\n",
    "    # Bounds is (0, 838019/5)\n",
    "    # need to skip N_ADDITIONAL at a time\n",
    "    i = start\n",
    "    while i < end:\n",
    "        print(i//(N_ADDITIONAL+1))\n",
    "        og_index = i//(N_ADDITIONAL+1)\n",
    "        # assign initial row\n",
    "        big_data.iloc[i] = new_df.iloc[og_index]\n",
    "        \n",
    "        # add N_ADDITIONAL new rows of data\n",
    "        j = 1\n",
    "        while i+j < end and j < N_ADDITIONAL+1:\n",
    "            big_data.iloc[i+j] = get_artificial_row(new_df.iloc[og_index])\n",
    "            j += 1\n",
    "        \n",
    "        # move ahead N_ADDITIONAL\n",
    "        i += N_ADDITIONAL+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = pd.DataFrame([[0]*len(new_df.columns) for _ in range(10)], columns=new_df.columns)\n",
    "#multi_thread(len(test), 1, apply_artificial_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rows(df, bounds):\n",
    "    start, end = bounds\n",
    "    for row_idx in range(start, end):\n",
    "        print_row(df.iloc[row_idx])\n",
    "\n",
    "def print_row(row):\n",
    "    for item in row:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbig_data = [[0]*old_n_cols for _ in range(new_n_rows)]\\ni, jump = 0, len(big_data)//10\\n\\nbig_data_1 = big_data.iloc[i:i+jump]\\ni += jump\\nbig_data_2 = big_data.iloc[i:i+jump]\\ni += jump\\nbig_data_3 = big_data.iloc[i:i+jump]\\ni += jump\\nbig_data_4 = big_data.iloc[i:i+jump]\\ni += jump\\nbig_data_5 = big_data.iloc[i:i+jump]\\ni += jump\\nbig_data_6 = big_data.iloc[i:i+jump]\\ni += jump\\nbig_data_7 = big_data.iloc[i:i+jump]\\ni += jump\\nbig_data_8 = big_data.iloc[i:i+jump]\\ni += jump\\nbig_data_9 = big_data.iloc[i:i+jump]\\ni += jump\\nbig_data_10 = big_data.iloc[i:]\\ni += jump\\nbig_data_1.to_csv('big_data_1.csv')\\nbig_data_2.to_csv('big_data_2.csv')\\nbig_data_3.to_csv('big_data_3.csv')\\nbig_data_4.to_csv('big_data_4.csv')\\nbig_data_5.to_csv('big_data_5.csv')\\nbig_data_6.to_csv('big_data_6.csv')\\nbig_data_7.to_csv('big_data_7.csv')\\nbig_data_8.to_csv('big_data_8.csv')\\nbig_data_9.to_csv('big_data_9.csv')\\nbig_data_10.to_csv('big_data_10.csv')\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create 10 big data csvs\n",
    "\"\"\"\n",
    "big_data = [[0]*old_n_cols for _ in range(new_n_rows)]\n",
    "i, jump = 0, len(big_data)//10\n",
    "\n",
    "big_data_1 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_2 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_3 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_4 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_5 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_6 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_7 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_8 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_9 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_10 = big_data.iloc[i:]\n",
    "i += jump\n",
    "big_data_1.to_csv('big_data_1.csv')\n",
    "big_data_2.to_csv('big_data_2.csv')\n",
    "big_data_3.to_csv('big_data_3.csv')\n",
    "big_data_4.to_csv('big_data_4.csv')\n",
    "big_data_5.to_csv('big_data_5.csv')\n",
    "big_data_6.to_csv('big_data_6.csv')\n",
    "big_data_7.to_csv('big_data_7.csv')\n",
    "big_data_8.to_csv('big_data_8.csv')\n",
    "big_data_9.to_csv('big_data_9.csv')\n",
    "big_data_10.to_csv('big_data_10.csv')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import pandas as pd\n",
    "big_data_1 = pd.read_csv('big_data_1.csv')\n",
    "big_data_2 = pd.read_csv('big_data_2.csv')\n",
    "big_data_3 = pd.read_csv('big_data_3.csv')\n",
    "big_data_4 = pd.read_csv('big_data_4.csv')\n",
    "big_data_5 = pd.read_csv('big_data_5.csv')\n",
    "big_data_6 = pd.read_csv('big_data_6.csv')\n",
    "big_data_7 = pd.read_csv('big_data_7.csv')\n",
    "big_data_8 = pd.read_csv('big_data_8.csv')\n",
    "big_data_9 = pd.read_csv('big_data_9.csv')\n",
    "big_data_10 = pd.read_csv('big_data_10.csv')\n",
    "\n",
    "big_data = big_data_1.append(big_data_2, ignore_index=True).append(big_data_3, ignore_index=True).append(big_data_4, ignore_index=True).append(big_data_5, ignore_index=True).append(big_data_6, ignore_index=True).append(big_data_7, ignore_index=True).append(big_data_8, ignore_index=True).append(big_data_9, ignore_index=True).append(big_data_10, ignore_index=True)\n",
    "del big_data['Unnamed: 0']'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multi_thread' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-040393a57b3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m### Run this for artificial data creation!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmulti_thread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbig_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapply_artificial_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'multi_thread' is not defined"
     ]
    }
   ],
   "source": [
    "### Run this for artificial data creation!\n",
    "# multi_thread(len(big_data), 1, apply_artificial_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "big_data_1 = pd.read_csv('art_data_1.csv')\n",
    "big_data_2 = pd.read_csv('art_data_2.csv')\n",
    "big_data_3 = pd.read_csv('art_data_3.csv')\n",
    "big_data_4 = pd.read_csv('art_data_4.csv')\n",
    "big_data_5 = pd.read_csv('art_data_5.csv')\n",
    "big_data_6 = pd.read_csv('art_data_6.csv')\n",
    "big_data_7 = pd.read_csv('art_data_7.csv')\n",
    "big_data_8 = pd.read_csv('art_data_8.csv')\n",
    "big_data_9 = pd.read_csv('art_data_9.csv')\n",
    "big_data_10 = pd.read_csv('art_data_10.csv')\n",
    "\n",
    "big_data = big_data_1.append(big_data_2, ignore_index=True).append(big_data_3, ignore_index=True).append(big_data_4, ignore_index=True).append(big_data_5, ignore_index=True).append(big_data_6, ignore_index=True).append(big_data_7, ignore_index=True).append(big_data_8, ignore_index=True).append(big_data_9, ignore_index=True).append(big_data_10, ignore_index=True)\n",
    "del big_data['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: execute after artificial data finishes!\n",
    "NUM_FILES = 20\n",
    "i, jump = 0, len(big_data)//NUM_FILES\n",
    "\n",
    "big_data_1 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_2 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_3 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_4 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_5 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_6 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_7 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_8 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_9 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_10 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_11 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_12 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_13 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_14 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_15 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_16 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_17 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_18 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_19 = big_data.iloc[i:i+jump]\n",
    "i += jump\n",
    "big_data_20 = big_data.iloc[i:]\n",
    "\n",
    "big_data_1.to_csv('art_data_1.csv')\n",
    "big_data_2.to_csv('art_data_2.csv')\n",
    "big_data_3.to_csv('art_data_3.csv')\n",
    "big_data_4.to_csv('art_data_4.csv')\n",
    "big_data_5.to_csv('art_data_5.csv')\n",
    "big_data_6.to_csv('art_data_6.csv')\n",
    "big_data_7.to_csv('art_data_7.csv')\n",
    "big_data_8.to_csv('art_data_8.csv')\n",
    "big_data_9.to_csv('art_data_9.csv')\n",
    "big_data_10.to_csv('art_data_10.csv')\n",
    "big_data_11.to_csv('art_data_11.csv')\n",
    "big_data_12.to_csv('art_data_12.csv')\n",
    "big_data_13.to_csv('art_data_13.csv')\n",
    "big_data_14.to_csv('art_data_14.csv')\n",
    "big_data_15.to_csv('art_data_15.csv')\n",
    "big_data_16.to_csv('art_data_16.csv')\n",
    "big_data_17.to_csv('art_data_17.csv')\n",
    "big_data_18.to_csv('art_data_18.csv')\n",
    "big_data_19.to_csv('art_data_19.csv')\n",
    "big_data_20.to_csv('art_data_20.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf, X_test, y_test = build_model(big_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7032617155255773\n"
     ]
    }
   ],
   "source": [
    "print_accuracy(y_test, model_predict(clf, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model-08-30.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(clf, 'model-08-30.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
